{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129339f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (9590, 224, 224, 3)\n",
      "Shape of Y_train_categorical: (9590, 5)\n",
      "Shape of X_test: (1686, 224, 224, 3)\n",
      "Shape of Y_test_categorical: (1686, 5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the paths to your dataset folders\n",
    "train_dataset_dir = \"/Users/sadianasrintisha/Desktop/Dataset/Ulcerative Colitis (LIMUC) Dataset/train_and_validation_sets\"\n",
    "test_dataset_dir = \"/Users/sadianasrintisha/Desktop/Dataset/Ulcerative Colitis (LIMUC) Dataset/test_set\"\n",
    "\n",
    "# Initialize empty lists for X_train, Y_train, X_test, and Y_test\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "# Initialize an empty list to store categorical labels\n",
    "categorical_labels = []\n",
    "\n",
    "# Define a function to read and preprocess images\n",
    "def process_images(folder_path, label, is_train_set=True):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".bmp\"):  # Check if it's a file and ends with .bmp\n",
    "            # Open and resize the image to (224,224, 3)\n",
    "            img = Image.open(file_path)\n",
    "            img = img.resize((224, 224))\n",
    "            img = img.convert(\"RGB\")\n",
    "            \n",
    "            # Convert image data to a NumPy array\n",
    "            img_array = np.array(img).astype('float32')  # Convert to float\n",
    "            \n",
    "            # Normalize the image data (optional)\n",
    "            img_array /= 255.0  # Normalize pixel values to [0, 1]\n",
    "            \n",
    "            # Append the image data to the appropriate list\n",
    "            if is_train_set:\n",
    "                X_train.append(img_array)\n",
    "                Y_train.append(label)  # Append the numerical label\n",
    "            else:\n",
    "                X_test.append(img_array)\n",
    "                Y_test.append(label)  # Append the numerical label\n",
    "            \n",
    "            # Append the label for categorical encoding\n",
    "            categorical_labels.append(label)  # Append the numerical label\n",
    "\n",
    "# List the folders inside the training dataset directory\n",
    "train_folders = os.listdir(train_dataset_dir)\n",
    "\n",
    "# Create a label encoder for categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Loop through the training folders and process images\n",
    "for label, folder_name in enumerate(train_folders):\n",
    "    folder_path = os.path.join(train_dataset_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        process_images(folder_path, label)\n",
    "\n",
    "# List the folders inside the test dataset directory\n",
    "test_folders = os.listdir(test_dataset_dir)\n",
    "\n",
    "# Loop through the test folders and process images\n",
    "for label, folder_name in enumerate(test_folders):\n",
    "    folder_path = os.path.join(test_dataset_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        process_images(folder_path, label, is_train_set=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# Encode Y_train and Y_test categorically\n",
    "num_classes = len(np.unique(categorical_labels))\n",
    "  # Update this to match the number of classes\n",
    "\n",
    "# Convert Y_train and Y_test to NumPy arrays\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Make sure your labels are integers ranging from 0 to num_classes - 1\n",
    "Y_train = Y_train.astype(int)\n",
    "Y_test = Y_test.astype(int)\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y_train_categorical = to_categorical(Y_train, num_classes=num_classes)\n",
    "Y_test_categorical = to_categorical(Y_test, num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "# # Encode Y_train and Y_test categorically using the label encoder\n",
    "# num_classes = len(np.unique(Y_train))  # Automatically determine the number of classes\n",
    "# Y_train_categorical = to_categorical(Y_train, num_classes=num_classes)\n",
    "# Y_test_categorical = to_categorical(Y_test, num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Check the shape of X_train, Y_train_categorical, X_test, and Y_test_categorical\n",
    "print(\"Shape of X_train:\", np.shape(X_train))\n",
    "print(\"Shape of Y_train_categorical:\", np.shape(Y_train_categorical))\n",
    "print(\"Shape of X_test:\", np.shape(X_test))\n",
    "print(\"Shape of Y_test_categorical:\", np.shape(Y_test_categorical))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72ef55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam as LegacyAdam\n",
    "\n",
    "# Create the ResNet model without the top (fully connected) layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "# Add the top layers for classification\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Change units to match the number of classes\n",
    "optimizer = LegacyAdam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train_categorical, Y_valid_categorical = train_test_split(\n",
    "    X_train, Y_train_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create tf.data.Dataset for training and validation data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train_categorical))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid_categorical))\n",
    "\n",
    "# Define batch size and shuffle the datasets\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
    "valid_dataset = valid_dataset.batch(batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cfd9577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "120/120 [==============================] - 2218s 18s/step - loss: 0.8207 - accuracy: 0.6724 - val_loss: 1.7828 - val_accuracy: 0.5386\n",
      "Epoch 2/50\n",
      "120/120 [==============================] - 2387s 20s/step - loss: 0.5917 - accuracy: 0.7424 - val_loss: 18.5093 - val_accuracy: 0.5386\n",
      "Epoch 3/50\n",
      "120/120 [==============================] - 3030s 25s/step - loss: 0.5364 - accuracy: 0.7656 - val_loss: 2.8504 - val_accuracy: 0.5386\n",
      "Epoch 4/50\n",
      "120/120 [==============================] - 3067s 26s/step - loss: 0.4886 - accuracy: 0.7891 - val_loss: 1.4319 - val_accuracy: 0.4473\n",
      "Epoch 5/50\n",
      "120/120 [==============================] - 1198s 10s/step - loss: 0.4421 - accuracy: 0.8137 - val_loss: 1.2742 - val_accuracy: 0.5547\n",
      "Epoch 6/50\n",
      "120/120 [==============================] - 1951s 16s/step - loss: 0.4140 - accuracy: 0.8226 - val_loss: 2.6391 - val_accuracy: 0.2847\n",
      "Epoch 7/50\n",
      "120/120 [==============================] - 1088s 9s/step - loss: 0.3425 - accuracy: 0.8596 - val_loss: 1.2000 - val_accuracy: 0.5266\n",
      "Epoch 8/50\n",
      "120/120 [==============================] - 1128s 9s/step - loss: 0.3050 - accuracy: 0.8767 - val_loss: 2.0668 - val_accuracy: 0.6069\n",
      "Epoch 9/50\n",
      "120/120 [==============================] - 1300s 11s/step - loss: 0.2464 - accuracy: 0.9045 - val_loss: 3.6977 - val_accuracy: 0.5474\n",
      "Epoch 10/50\n",
      "120/120 [==============================] - 1288s 11s/step - loss: 0.2004 - accuracy: 0.9210 - val_loss: 1.0658 - val_accuracy: 0.6220\n",
      "Epoch 11/50\n",
      "120/120 [==============================] - 1425s 12s/step - loss: 0.1680 - accuracy: 0.9385 - val_loss: 5.4390 - val_accuracy: 0.5683\n",
      "Epoch 12/50\n",
      "120/120 [==============================] - 20765s 174s/step - loss: 0.1415 - accuracy: 0.9481 - val_loss: 2.9679 - val_accuracy: 0.6058\n",
      "Epoch 13/50\n",
      "120/120 [==============================] - 1125s 9s/step - loss: 0.1358 - accuracy: 0.9502 - val_loss: 1.3064 - val_accuracy: 0.6575\n",
      "Epoch 14/50\n",
      "120/120 [==============================] - 2436s 20s/step - loss: 0.0818 - accuracy: 0.9725 - val_loss: 1.2114 - val_accuracy: 0.6330\n",
      "Epoch 15/50\n",
      "120/120 [==============================] - 5880s 49s/step - loss: 0.0669 - accuracy: 0.9761 - val_loss: 1.7928 - val_accuracy: 0.6387\n",
      "Epoch 16/50\n",
      "120/120 [==============================] - 5120s 43s/step - loss: 0.0689 - accuracy: 0.9765 - val_loss: 1.9264 - val_accuracy: 0.6496\n",
      "Epoch 17/50\n",
      "120/120 [==============================] - 3284s 28s/step - loss: 0.0792 - accuracy: 0.9728 - val_loss: 2.0801 - val_accuracy: 0.6163\n",
      "Epoch 18/50\n",
      "120/120 [==============================] - 9962s 84s/step - loss: 0.0717 - accuracy: 0.9741 - val_loss: 2.3512 - val_accuracy: 0.6507\n",
      "Epoch 19/50\n",
      "120/120 [==============================] - 1403s 12s/step - loss: 0.0655 - accuracy: 0.9782 - val_loss: 2.0946 - val_accuracy: 0.6298\n",
      "Epoch 20/50\n",
      "120/120 [==============================] - 4177s 35s/step - loss: 0.0468 - accuracy: 0.9840 - val_loss: 1.3404 - val_accuracy: 0.6846\n",
      "Epoch 21/50\n",
      "120/120 [==============================] - 5308s 45s/step - loss: 0.0413 - accuracy: 0.9847 - val_loss: 3.0369 - val_accuracy: 0.6387\n",
      "Epoch 22/50\n",
      "120/120 [==============================] - 1666s 14s/step - loss: 0.0695 - accuracy: 0.9759 - val_loss: 2.5493 - val_accuracy: 0.6074\n",
      "Epoch 23/50\n",
      "120/120 [==============================] - 1297s 11s/step - loss: 0.0701 - accuracy: 0.9768 - val_loss: 2.3527 - val_accuracy: 0.5777\n",
      "Epoch 24/50\n",
      "120/120 [==============================] - 4141s 35s/step - loss: 0.0630 - accuracy: 0.9788 - val_loss: 4.0017 - val_accuracy: 0.5688\n",
      "Epoch 25/50\n",
      "120/120 [==============================] - 1069s 9s/step - loss: 0.0516 - accuracy: 0.9836 - val_loss: 1.9731 - val_accuracy: 0.5584\n",
      "Epoch 26/50\n",
      "120/120 [==============================] - 1072s 9s/step - loss: 0.0428 - accuracy: 0.9874 - val_loss: 1.7955 - val_accuracy: 0.6220\n",
      "Epoch 27/50\n",
      "120/120 [==============================] - 3252s 27s/step - loss: 0.0268 - accuracy: 0.9914 - val_loss: 2.2951 - val_accuracy: 0.6298\n",
      "Epoch 28/50\n",
      "120/120 [==============================] - 966s 8s/step - loss: 0.0432 - accuracy: 0.9849 - val_loss: 7.3401 - val_accuracy: 0.3014\n",
      "Epoch 29/50\n",
      "120/120 [==============================] - 1025s 9s/step - loss: 0.0408 - accuracy: 0.9866 - val_loss: 4.5805 - val_accuracy: 0.6017\n",
      "Epoch 30/50\n",
      "120/120 [==============================] - 1064s 9s/step - loss: 0.0459 - accuracy: 0.9836 - val_loss: 1.5444 - val_accuracy: 0.6413\n",
      "Epoch 31/50\n",
      "120/120 [==============================] - 9313s 78s/step - loss: 0.0299 - accuracy: 0.9889 - val_loss: 2.1834 - val_accuracy: 0.6455\n",
      "Epoch 32/50\n",
      "120/120 [==============================] - 27310s 213s/step - loss: 0.0228 - accuracy: 0.9931 - val_loss: 2.3808 - val_accuracy: 0.6773\n",
      "Epoch 33/50\n",
      "120/120 [==============================] - 2152s 18s/step - loss: 0.0262 - accuracy: 0.9897 - val_loss: 1.7715 - val_accuracy: 0.6663\n",
      "Epoch 34/50\n",
      "120/120 [==============================] - 4147s 35s/step - loss: 0.0513 - accuracy: 0.9821 - val_loss: 3.2735 - val_accuracy: 0.6064\n",
      "Epoch 35/50\n",
      "120/120 [==============================] - 1317s 11s/step - loss: 0.0473 - accuracy: 0.9833 - val_loss: 1.8176 - val_accuracy: 0.6314\n",
      "Epoch 36/50\n",
      "120/120 [==============================] - 1498s 12s/step - loss: 0.0418 - accuracy: 0.9863 - val_loss: 2.0300 - val_accuracy: 0.6131\n",
      "Epoch 37/50\n",
      "120/120 [==============================] - 1519s 13s/step - loss: 0.0297 - accuracy: 0.9891 - val_loss: 3.1751 - val_accuracy: 0.6767\n",
      "Epoch 38/50\n",
      "120/120 [==============================] - 1511s 13s/step - loss: 0.0316 - accuracy: 0.9897 - val_loss: 1.8095 - val_accuracy: 0.6788\n",
      "Epoch 39/50\n",
      "120/120 [==============================] - 1526s 13s/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 1.9475 - val_accuracy: 0.6679\n",
      "Epoch 40/50\n",
      "120/120 [==============================] - 1545s 13s/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 1.5489 - val_accuracy: 0.6945\n",
      "Epoch 41/50\n",
      "120/120 [==============================] - 1562s 13s/step - loss: 0.0445 - accuracy: 0.9842 - val_loss: 1.5448 - val_accuracy: 0.6564\n",
      "Epoch 42/50\n",
      "120/120 [==============================] - 4983s 42s/step - loss: 0.0537 - accuracy: 0.9824 - val_loss: 1.5008 - val_accuracy: 0.6694\n",
      "Epoch 43/50\n",
      "120/120 [==============================] - 2364s 20s/step - loss: 0.0322 - accuracy: 0.9891 - val_loss: 2.5602 - val_accuracy: 0.6580\n",
      "Epoch 44/50\n",
      "120/120 [==============================] - 1340s 11s/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 1.9141 - val_accuracy: 0.6460\n",
      "Epoch 45/50\n",
      "120/120 [==============================] - 1517s 13s/step - loss: 0.0266 - accuracy: 0.9906 - val_loss: 1.5648 - val_accuracy: 0.7070\n",
      "Epoch 46/50\n",
      "120/120 [==============================] - 1567s 13s/step - loss: 0.0355 - accuracy: 0.9885 - val_loss: 9.6180 - val_accuracy: 0.5527\n",
      "Epoch 47/50\n",
      "120/120 [==============================] - 4476s 38s/step - loss: 0.0390 - accuracy: 0.9858 - val_loss: 2.4830 - val_accuracy: 0.6507\n",
      "Epoch 48/50\n",
      "120/120 [==============================] - 4023s 34s/step - loss: 0.0382 - accuracy: 0.9859 - val_loss: 2.5796 - val_accuracy: 0.6309\n",
      "Epoch 49/50\n",
      "120/120 [==============================] - 1049s 9s/step - loss: 0.0240 - accuracy: 0.9914 - val_loss: 1.5345 - val_accuracy: 0.6648\n",
      "Epoch 50/50\n",
      "120/120 [==============================] - 1935s 16s/step - loss: 0.0187 - accuracy: 0.9932 - val_loss: 1.6972 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17ff59210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 50\n",
    "model.fit(train_dataset, epochs=epochs, validation_data=valid_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2a4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 48s 2s/step - loss: 7.9652 - accuracy: 0.1999\n",
      "27/27 [==============================] - 48s 2s/step\n",
      "Test Loss: 7.965188503265381\n",
      "Test Accuracy: 0.19988137483596802\n",
      "Accuracy: 0.19988137603795966\n",
      "F1 Score: 0.2148435746893601\n",
      "Confusion Matrix:\n",
      "[[  0 338  73  50   3]\n",
      " [  0 282 634   8   1]\n",
      " [  0  58   8  83  28]\n",
      " [  0  15   2  47  56]\n",
      " [  0   0   0   0   0]]\n",
      "cohen_kappa_score :\n",
      "0.3712505639313779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test_categorical))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Convert one-hot encoded predictions back to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(Y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test, predicted_labels)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"cohen_kappa_score :\")\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "k = cohen_kappa_score(Y_test, predicted_labels, weights='quadratic')\n",
    "print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c7b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30219e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c1011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
